
from typing import List, Set
from toolguard.data_types import Domain, FileTwin, ToolPolicyItem
from programmatic_ai import generative
from pydantic import BaseModel, Field
import re

PYTHON_PATTERN = r'^```python\s*\n[\s\S]*?\n```$'
class PythonCodeModel(BaseModel):
    python_code: str = Field(
        ...,
        pattern=PYTHON_PATTERN
    )
    def get_code_content(self) -> str:
        """
        Extracts the Python code content from the markdown-style code block.
        Returns:
            str: The inner Python code without the surrounding markdown syntax.
        """
        match = re.match(PYTHON_PATTERN, self.python_code)
        if match:
            return match.group(1)\
                .replace("\\n", "\n")
        raise ValueError("Invalid python_code format")

@generative
async def generate_tool_item_tests(
    fn_under_test_name: str, 
    fn_src: FileTwin, 
    tool_item: ToolPolicyItem, 
    domain: Domain, 
    dependent_tool_names: Set[str])-> PythonCodeModel:
    """
    Generate Python unit tests for a function to verify tool-call compliance with policy constraints.

    This function creates unit tests to validate the behavior of a given function-under-test. 
    The function goal is to check the argument data, and raise an exception if they violated the requirements in the policy item.

    **Test Generation Rules:**
    - Make sure to Python import all items in fn_src, common and domain modules. Example:
    - Each **policy item** has multiple **compliance** and **violation** test examples.
        - For **each compliance example, ONE test method** is generated. 
            - If an exception occurrs in the function-under-test, let the exception propagate up.
        - For **each violation example, ONE test** is generated.
            - The function-under-test is EXPECTED to raise a `PolicyViolationException`.
            - If the expected exception was not raised, the test should raise an exception with a message describing test case that did not raise exception.
            
        - Test class and method names should be meaningful and use up to **six words in snake_case**.
        - For each test, add a comment quoting the policy item case that this function is testing 
        - Failure message should describe the test scenario that failed, the expected and the actual outcomes.

    **Data population and references:**
    - When populating domain objects, use pydantic `.model_construct()`. 
    - If the class extends `pydantic.RootModel`, always pass the `root` argument.
    - You should mock the return_value from **ALL tools listed in `dependent_tool_names`**. 
        - Use side_effect to return the expected value only when the expected parameters are passed.
    - You should mock the chat_history services. 
    
    **Example:** Testing the function `check_create_reservation`, 
    * Policy: `cannot book a room for a date in the past`
    * Violation example: `book a room for a hotel, one week ago`
    * Dependent_tool_names: `["get_user", "get_hotel"]`
    * Domain: 
```python
# file: my_app/api.py
class SomeAPI(ABC):
    def get_user(self, user_id):
        ...
    def get_hotel(self, hotel_id):
        ...
    def create_reservation(self, args: Reservation):
        ...
```
    * fn_under_test_name: `guard_create_reservation`
    * fn_src:
```python
# file: my_app/guard_create_reservation.py
def guard_create_reservation(args:Reservation, history: ChatHistory, api: SomeAPI):
    ...
```

    Should generate this snippet:
```python
from unittest.mock import MagicMock, patch
from toolguard.data_types import PolicyViolationException
from my_app.guard_create_reservation import guard_create_reservation
from my_app.api import *

def test_book_in_the_past():
    # Policy: "cannot book room for a date in the past"
    # Example: "book a room for a hotel, one week ago"
    
    # mock the history service:
    history = MagicMock()
    history.ask_bool.return_value = True #Mock that True is the answer to the question

    # mock other tools function return values 
    user = User.model_construct(user_id=123, ...)
    hotel = Hotel.model_construct(hotel_id="789", ...)

    api = MagicMock()
    api.get_user.return_value = lambda user_id: user if user_id == 123 else None
    api.get_hotel.return_value = lambda hotel_id: hotel if hotel_id == "789" else None
    
    #invoke function under test.
    check_create_reservation(args, history, api)
```

    Args:
        fn_under_test_name (str): the name of the function under test
        fn_src (FileTwin): Source code containing the function-under-test signature.
        tool_item (ToolPolicyItem): Specification of the function-under-test, including positive and negative examples.
        domain (Domain): available data types and interfaces the test can use.
        dependent_tool_names(Set[str]): other tool names that this tool depends on

    Returns:
        str: Generated Python unit test code.
    """
    ...


@generative
async def improve_tool_tests(
    prev_impl: PythonCodeModel, 
    domain: Domain, 
    policy_item: ToolPolicyItem, 
    review_comments: List[str])-> PythonCodeModel:
    """
    Improve the previous test functions (in Python) to check the given tool policy-items according to the review-comments.
    **Implementation Rules:**"
    - Do not change the function signature.
    - You can add import statements, but ont remove them.

    Args:
        prev_impl (FileTwin): previous implementation of a Python function.
        domain (Domain): Python source code defining available data types and APIs that the test can use.
        tool (ToolPolicyItem): Requirements for this tool.
        review_comments (List[str]): Review comments on the current implementation. For example, pylint errors or Failed unit-tests.

    Returns:
        str: Improved implementation pytest test functions.
    """
    ...


@generative
async def tool_information_dependencies(tool_name:str, policy: str, domain:FileTwin)-> List[str]:
    """
    List other tools that the given tool depends on.

    Args:
        tool_name (str): name of the tool under analysis
        policy (str): business policy, in natural language, specifying a constraint on a business processes involving the tool unedr analysis
        domain (FileTwin): Python code defining available data types and other tool interfaces

    Returns:
        Set[str]: dependent tool names

    **Dependency Rules:**
    - Tool available information is: from it function arguments, or from calling other tools.
    - The function analyzes information dependency only on other tools.
    - Information dependency can be only on tools that are immutable. That is, that retieve data only, but do not modify the environment.
    - A dependency in another-tool exists only if the policy mention information is not available in the arguments, but can accessed by calling the other tool.
    - The set of dependencies can be empty, with one, or multiple tools.
    
    **Example: ** 
```
    domain = {
        "file_name": "domain.py",
        "content": "
    class Car:
        pass
    class Person:
        pass
    class Insurance:
        pass
    class CarAPI(Protocol):
        def buy_car(self, car:Car, owner_id:str, insurance_id:str):
            pass
        def get_person(self, id:str) -> Person:
            pass
        def get_insurance(self, id:str) -> Insurance:
            pass
        def delete_car(self, car:Car):
            pass
        def list_all_cars_owned_by(self, id:str): List[Car]
            pass
    "
    }

    # According to the policy, the `buy_car` operation depends on `get_person` to get the driving licence of the owner, and on `get_insurance` to check the insurance is valid.
    assert tool_dependencies(
        "tool_name": "buy_car", 
        "policy": "when buying a new car, you should check that the car owner has a driving licence and that the insurance is valid.",
        "domain": domain
    ) == {"get_person", "get_insurance"}

    # According to the policy, the `get_insurance` operation does not depend on any other operation
    assert tool_dependencies(
        "tool_name": "get_insurance", 
        "policy": "when buying a new car, you should check that the car owner has a driving licence and that the insurance is valid.",
        "domain": domain
    ) == {}

    # According to the policy, the `delete_car` operation does not depend on any other operation
    assert tool_dependencies(
        "tool_name": "delete_car", 
        "policy": "when buying a new car, you should check that the car owner has a driving licence and that the insurance is valid.",
        "domain": domain
    ) == {}

    ```

    """
    ...


@generative
async def improve_tool_guard_fn(prev_impl: PythonCodeModel, domain: Domain, policy_item: ToolPolicyItem, review_comments: List[str])-> PythonCodeModel:
    """
    Improve the previous tool-call guard implementation (in Python) to cover all tool policy-items according to the review-comments.

    Args:
        prev_impl (FileTwin): previous implementation of the tool-call check.
        domain (Domain): Python code defining available data types and other tool interfaces.
        policy_item (ToolPolicyItem): Requirements for this tool.
        review_comments (List[str]): Review comments on the current implementation. For example, pylint errors or Failed unit-tests.

    Returns:
        str: Improved implementation of the tool-call check.

    **Implementation Rules:**"
    - Do not change the function signature.
    - ALL tool policy items must be validated on the tool arguments.
    - The code should be simple.
    - The code should be well documented.
    - You should just validate the tool-call. You should never call the tool itself.
    - If needed, you may use the `api` interface to get additional information from the backend.
    - If needed, you may call `history.ask(question)` or `history.ask_bool(question)` services to check if some interaction happened in this chat. Your question should be clear. For example: "did the user confirm the agent suggestion?".
    - History services are slow and expensive. Prefer calling domain functions over history services.
    """
    ...