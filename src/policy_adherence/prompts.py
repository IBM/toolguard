
from typing import List, Set
from policy_adherence.types import SourceFile, ToolPolicyItem
from programmatic_ai import generative

model = "gpt-4o-2024-08-06"

@generative(model=model, provider="azure", sdk="litellm")
def generate_tool_item_tests(
    fn_under_test_name:str, 
    fn_src:SourceFile, 
    tool_item:ToolPolicyItem, 
    common: SourceFile, 
    domain:SourceFile, 
    dependent_tool_names: Set[str])-> str:
    """Generate Python unit tests for a function to verify tool-call compliance with policy constraints.

    This function creates unit tests to validate the behavior of a given function-under-test. 
    The function goal is to check the argument data, and raise an exception if they violated the requirements in the policy item.

    **Test Generation Rules:**
    - Make sure to Python import all items in fn_src, common and domain modules. Example:
```
from check_my_fn import check_my_function
from common import *
from domain import *
```
    - Each **policy item** has multiple **compliance** and **violation** test examples.
        - For **each compliance example, ONE test method** is generated. 
            - If an exception occurrs in the function-under-test, let the exception propagate up.
        - For **each violation example, ONE test** is generated.
            - the function-under-test is EXPECTED to raise an exception.
            - dont expect a particular error message
        - Test class and method names should be meaningful and use up to **six words in snake_case**.
        - For each test, add a comment quoting the policy item case that this function is testing 
        - Failure message should describe the test scenario that failed, the expected and the actual outcomes.

    **Data population and references:**
    - populate all request object fields that are required by Pydantic or that are required according to the policy.
    - When populating domain objects, use pydantic `.model_construct()`.
    - You should mock the return_value from **ALL tools listed in `dependent_tool_names`**.
    - You should mock the chat_history services. 
    
    **Example:** Testing the function `check_create_reservation`, for policy: `cannot book room for a date in the past`, and mocking dependent_tool_names: `["get_user", "get_hotel"]`
    ```python

    from unittest.mock import MagicMock, patch

    # domain:
    def get_user(user_id):
        ...
    def get_hotel(hotel_id):
        ...

    # function call arguments
    args = {
        user_id: ...,
        ...
        hotel_id: ....
    }

    # mock return values:
    user = User.model_construct(...)
    hotel = Hotel.model_construct(...)

    # mock the history service:
    history = MagicMock()
    history.ask_bool.return_value = True #Mock that True is the answer to the question

    # mock functions and invoke function under test.

    with patch("check_create_reservation.get_user", return_value=user):
        with patch("check_create_reservation.get_hotel", return_value=hotel):
            try:
                check_book_flight(args, history)
            except Exception as ex:
                # failure message describe the test case
                pytest.fail("The user cannot book room for a date in the past")
    ```

    Args:
        fn_under_test_name (str): the name of the function under test
        fn_src (SourceFile): Source code containing the function-under-test signature.
        tool (ToolPolicy): Specification of the function-under-test, including positive and negative examples.
        common (SourceFile): utility functions the test may use
        domain (SourceFile): available data types and interfaces needed by the tests.
        dependent_tool_names(Set[str]): other tool names that this tool depends on

    Returns:
        str: Generated Python unit test code.
    """
    ...


@generative(model=model, provider="azure", sdk="litellm")
def improve_tool_tests(prev_impl:SourceFile, domain: SourceFile, tool: ToolPolicyItem, review_comments: List[str])-> str:
    """
    Improve the previous test functions (in Python) to check the given tool policy-items according to the review-comments.

    Args:
        prev_impl (SourceFile): previous implementation of a Python function.
        domain (SourceFile): Python code defining available data types and other tool interfaces.
        tool (ToolPolicy): Requirements for this tool.
        review_comments (List[str]): Review comments on the current implementation. For example, pylint errors or Failed unit-tests.

    Returns:
        str: Improved implementation pytest test functions.
    """
    ...


@generative(model=model, provider="azure", sdk="litellm")
def tool_information_dependencies(tool_name:str, policy: str, domain:SourceFile)-> List[str]:
    """
    List other tools that the given tool depends on.

    Args:
        tool_name (str): name of the tool under analysis
        policy (str): business policy, in natural language, specifying a constraint on a business processes involving the tool unedr analysis
        domain (SourceFile): Python code defining available data types and other tool interfaces

    Returns:
        List[str]: dependent tool names

    **Dependency Rules:**
    - Tool available information is: from it function arguments, or from calling other tools.
    - The function analyzes information dependency only on other tools.
    - Information dependency can be only on tools that are immutable. That is, that retieve data only, but do not modify the environment.
    - A dependency in another-tool exists only if the policy mention information is not available in the arguments, but can accessed by calling the other tool.
    - The list of dependencies can be empty, with one, or multiple tools.
    
    **Example: ** 
```
    domain = {
        "file_name": "domain.py",
        "content": "
    class Car:
        pass
    class Person:
        pass
    class Insurance:
        pass
    def buy_car(car:Car, owner_id:str, insurance_id:str):
        pass
    def get_person(id:str) -> Person:
        pass
    def get_insurance(id:str) -> Insurance:
        pass
    def delete_car(car:Car):
        pass
    def list_all_cars_owned_by(id:str): List[Car]
        pass
    "
    }

    # According to the policy, the `buy_car` operation depends on `get_person` to get the driving licence of the owner, and on `get_insurance` to check the insurance is valid.
    assert tool_dependencies(
        "tool_name": "buy_car", 
        "policy": "when buying a new car, you should check that the car owner has a driving licence and that the insurance is valid.",
        "domain": domain
    ) == ["get_person", "get_insurance"]

    # According to the policy, the `get_insurance` operation does not depend on any other operation
    assert tool_dependencies(
        "tool_name": "get_insurance", 
        "policy": "when buying a new car, you should check that the car owner has a driving licence and that the insurance is valid.",
        "domain": domain
    ) == []

    # According to the policy, the `delete_car` operation does not depend on any other operation
    assert tool_dependencies(
        "tool_name": "delete_car", 
        "policy": "when buying a new car, you should check that the car owner has a driving licence and that the insurance is valid.",
        "domain": domain
    ) == []

    ```

    """
    ...


@generative(model=model, provider="azure", sdk="litellm")
def improve_tool_check_fn(prev_impl:SourceFile, domain: SourceFile, policy_item: ToolPolicyItem, review_comments: List[str])-> str:
    """
    Improve the previous tool-call check implementation (in Python) to cover all tool policy-items according to the review-comments.

    Args:
        prev_impl (SourceFile): previous implementation of the tool-call check.
        domain (SourceFile): Python code defining available data types and other tool interfaces.
        policy_item (ToolPolicyItem): Requirements for this tool.
        review_comments (List[str]): Review comments on the current implementation. For example, pylint errors or Failed unit-tests.

    Returns:
        str: Improved implementation of the tool-call check.

    **Implementation Rules:**"
    - ALL tool policy items must be validated on the tool arguments.
    - The code should be simple.
    - The code should be well documented.
    - You should just validate the tool-call. You should never call the tool itself.
    - If needed, you may call other domain functions to get additional information from the backend. Make sure to import the other functions.
    - If needed, you may call `chat_history.ask(question)` or `chat_history.ask_bool(question)` services to check if some interaction happened in this chat. Your question should be clear. For example: "did the user confirm the agent suggestion?".
    - History services are slow and expensive. Prefer calling domain functions over history services.
    """
    ...