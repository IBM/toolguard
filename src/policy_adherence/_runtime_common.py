
import json
from typing import Dict, List

from pydantic import BaseModel

class LLM(BaseModel):
    def generate(self, messages: List[Dict])->str:
        ...

class Litellm(LLM):
    model_name: str
    custom_provider: str

    def __init__(self, model_name: str, custom_provider: str = "azure") -> None:
        super().__init__(model_name=model_name, custom_provider=custom_provider)
        
    def generate(self, messages: List[Dict])->str:
        from litellm import completion
        resp = completion(
            messages=messages,
            model=self.model_name,
            custom_llm_provider= self.custom_provider)
        return resp.choices[0].message.content
    
def ask_llm(question:str, conversation: List[Dict], llm: LLM)->str:
    prompt = f"""You are given a question and an historical conversation between a user and an ai-agent.
Your task is to answer the question according to the conversation.

Conversation:
{json.dumps(conversation, indent=4)}

Question:
{question}
"""
    msg = {"role":"system", "content": prompt}
    return llm.generate([msg])

class ChatHistory:
    """Represents a history of chat messages and provides methods check if specific events already happened."""
    messages: List[Dict]
    llm: LLM

    def __init__(self, messages: List[Dict], llm: LLM) -> None:
        self.messages = messages
        self.llm = llm

    def ask(self, question:str)->str:
        """Asks a question using the chat history and returns the model's textual response.

        Args:
            question (str): The question to be asked. Example: "What cancellation reason type did the user provide: 'Health', 'Change of plans', 'Other'"

        Returns:
            str: The response generated by the language model.
        """
        return ask_llm(question, self.messages, self.llm)
    
    def ask_bool(self, question:str)->bool:
        """Asks a yes/no question and returns the response as a boolean.

        Args:
            question (str): The yes/no question to be asked. Example: "Did the user accepted the agent's proposal?"

        Returns:
            bool: The interpreted boolean response from the language model.
        """
        return bool(ask_llm(question, self.messages, self.llm))
    

    def did_tool_return_value(self, tool_name:str,expected_value)->bool:
        """Checks whether a specific tool was called in the chat history and validates if the expected value was returned
            Example: "did_tool_return_value("book_hotel",True) checks if the history shows calling the function book_hotel and if the returned value was true did_tool_return_value will return true else false
       

        Args:
            tool_name (str): The name of the tool to check for in the message history.
            expected_value: The expected value of the tool call.
            
            

        Returns:
            bool: True if the tool was called returning expected_value, False otherwise.
        """
        for msg in self.messages:
            if msg.get('tool_name') == tool_name and msg.get('content') == expected_value:
                return True
        return False

    def was_tool_called(self, tool_name: str) -> bool:
        """Checks whether a specific tool was called in the chat history.
        
		Args:
			tool_name (str): The name of the tool to check for in the message history.

		Returns:
			bool: True if the tool was called, False otherwise.
		"""
        for msg in self.messages:
            if msg.get('tool_name') == tool_name:
                return True
        return False


class PolicyViolationException(Exception):
    _msg: str
    def __init__(self, message:str):
        super().__init__(message)
        self._msg = message

    @property
    def message(self):
        return self._msg